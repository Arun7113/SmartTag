{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# requirements:\n",
        "# pip install transformers datasets accelerate evaluate\n",
        "\n",
        "# Install specific version to avoid compatibility issues\n",
        "!pip install -q --upgrade transformers==4.45.0 huggingface_hub datasets accelerate evaluate scikit-learn\n",
        "\n",
        "import os\n",
        "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# GPU DETECTION AND CONFIGURATION\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"GPU DETECTION AND CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we're in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Running in Google Colab\")\n",
        "\n",
        "    # Check GPU type in Colab\n",
        "    gpu_info = !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
        "    if gpu_info:\n",
        "        print(f\"‚úÖ GPU: {gpu_info[0]}\")\n",
        "    else:\n",
        "        print(\"‚ùå No GPU detected in Colab\")\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ùå Not in Google Colab\")\n",
        "\n",
        "# PyTorch GPU detection\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "    print(f\"‚úÖ PyTorch GPU: {gpu_name}\")\n",
        "    print(f\"‚úÖ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "    # Enable performance optimizations\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes\n",
        "    print(\"‚úÖ GPU optimizations enabled: TF32, CuDNN benchmark\")\n",
        "\n",
        "    # Set GPU memory growth to avoid OOM\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU cache cleared\")\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ùå No GPU available - using CPU\")\n",
        "    print(\"‚ö†Ô∏è  Training will be significantly slower!\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Configuration - CHANGED TO RoBERTa\n",
        "MODEL_NAME = \"roberta-large\"\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 386\n",
        "BATCH_SIZE = 64 if torch.cuda.is_available() else 8  # Reduced for RoBERTa-large (larger model)\n",
        "EPOCHS = 70\n",
        "\n",
        "# Early Stopping Configuration - USING SMAPE\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "EARLY_STOPPING_THRESHOLD = 0.1\n",
        "\n",
        "# TEST MODE - Use only first 1000 samples\n",
        "TEST_MODE = False\n",
        "SAMPLE_SIZE = 1000\n",
        "\n",
        "# ============================================================================\n",
        "# SMAPE Calculation Functions\n",
        "# ============================================================================\n",
        "def smape_loss(y_true, y_pred):\n",
        "    \"\"\"Calculate SMAPE loss for PyTorch - optimized for GPU\"\"\"\n",
        "    return torch.mean(2 * torch.abs(y_pred - y_true) / (torch.abs(y_true) + torch.abs(y_pred) + 1e-8))\n",
        "\n",
        "def smape_metric(y_true, y_pred):\n",
        "    \"\"\"Calculate SMAPE metric for numpy arrays\"\"\"\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATA LOADING AND PREPARATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Arun_code/Amazon_Ml_2025/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Arun_code/Amazon_Ml_2025/test.csv\")  # Using train as test for validation\n",
        "\n",
        "if TEST_MODE:\n",
        "    print(f\"üöÄ TEST MODE: Using first {SAMPLE_SIZE} samples for quick validation\")\n",
        "    train_df = train_df.head(SAMPLE_SIZE).copy()\n",
        "    test_df = test_df.head(SAMPLE_SIZE).copy()\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "# Check data distribution\n",
        "print(f\"\\nData Overview:\")\n",
        "print(f\"Price statistics - Min: ${train_df['price'].min():.2f}, Max: ${train_df['price'].max():.2f}, Mean: ${train_df['price'].mean():.2f}\")\n",
        "print(f\"Catalog content length - Avg: {train_df['catalog_content'].str.len().mean():.1f} chars\")\n",
        "\n",
        "# Initialize tokenizer - CHANGED FOR RoBERTa\n",
        "print(\"\\nLoading RoBERTa tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    local_files_only=False\n",
        ")\n",
        "# Add RoBERTa special tokens\n",
        "if not tokenizer.cls_token:\n",
        "    tokenizer.add_special_tokens({'cls_token': '<s>'})  # RoBERTa uses <s> as CLS\n",
        "print(\"‚úÖ RoBERTa tokenizer loaded successfully!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Custom Model with Layer Unfreezing - MODIFIED FOR RoBERTa\n",
        "# ============================================================================\n",
        "class RobertaRegressionModel(nn.Module):  # CHANGED CLASS NAME\n",
        "    def __init__(self, model_name, unfreeze_last_n_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"\\nInitializing RoBERTa model on {device}...\")\n",
        "\n",
        "        # Load pretrained RoBERTa\n",
        "        self.roberta = AutoModel.from_pretrained(model_name)  # CHANGED TO RoBERTa\n",
        "\n",
        "        # Freeze all parameters first\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last N layers\n",
        "        if unfreeze_last_n_layers > 0:\n",
        "            # Unfreeze the last N transformer layers\n",
        "            for layer in self.roberta.encoder.layer[-unfreeze_last_n_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # RoBERTa doesn't have a pooler like BERT, so we'll use the first token representation\n",
        "        # Regression head - optimized for GPU\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.roberta.config.hidden_size, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Print model info\n",
        "        self._print_model_info(unfreeze_last_n_layers)\n",
        "\n",
        "    def _print_model_info(self, unfreeze_last_n_layers):\n",
        "        total_params = 0\n",
        "        trainable_params = 0\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            total_params += param.numel()\n",
        "            if param.requires_grad:\n",
        "                trainable_params += param.numel()\n",
        "\n",
        "        print(f\"\\nüìä MODEL ARCHITECTURE:\")\n",
        "        print(f\"   Model: RoBERTa-large\")\n",
        "        print(f\"   Device: {device}\")\n",
        "        print(f\"   Unfrozen layers: Last {unfreeze_last_n_layers} transformer layers\")\n",
        "        print(f\"   Total parameters: {total_params:,}\")\n",
        "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
        "        print(f\"   Loss function: SMAPE (competition metric)\")\n",
        "        print(f\"   Hidden size: {self.roberta.config.hidden_size}\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Get RoBERTa outputs\n",
        "        outputs = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Use the [CLS] token representation (first token) for regression\n",
        "        # RoBERTa doesn't have pooler_output like BERT\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Regression prediction\n",
        "        logits = self.regressor(cls_output).squeeze(-1)\n",
        "\n",
        "        return {'logits': logits}\n",
        "\n",
        "# Initialize Model\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL INITIALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model = RobertaRegressionModel(  # CHANGED TO RoBERTa MODEL\n",
        "    model_name=MODEL_NAME,\n",
        "    unfreeze_last_n_layers=2\n",
        ")\n",
        "\n",
        "# Move model to GPU with memory optimization\n",
        "model = model.to(device)\n",
        "print(f\"‚úÖ RoBERTa model successfully moved to {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Dataset Preparation with GPU Optimization\n",
        "# ============================================================================\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(\n",
        "        example[\"catalog_content\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "# For validation, split the training data\n",
        "train_pd, val_pd = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä DATASET SPLIT:\")\n",
        "print(f\"   Training samples: {len(train_pd)}\")\n",
        "print(f\"   Validation samples: {len(val_pd)}\")\n",
        "\n",
        "# Convert to Hugging Face datasets\n",
        "hf_train = Dataset.from_pandas(train_pd)\n",
        "hf_val = Dataset.from_pandas(val_pd)\n",
        "hf_test = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing datasets...\")\n",
        "hf_train = hf_train.map(tokenize_fn, batched=True, batch_size=32)  # Batched for speed\n",
        "hf_val = hf_val.map(tokenize_fn, batched=True, batch_size=32)\n",
        "hf_test = hf_test.map(tokenize_fn, batched=True, batch_size=32)\n",
        "\n",
        "# Rename price column to labels for training\n",
        "hf_train = hf_train.rename_column(\"price\", \"labels\")\n",
        "hf_val = hf_val.rename_column(\"price\", \"labels\")\n",
        "\n",
        "# Set format for PyTorch with GPU optimization\n",
        "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "hf_train.set_format(type=\"torch\", columns=cols)\n",
        "hf_val.set_format(type=\"torch\", columns=cols)\n",
        "hf_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "print(f\"‚úÖ Dataset preparation complete:\")\n",
        "print(f\"   Training samples: {len(hf_train)}\")\n",
        "print(f\"   Validation samples: {len(hf_val)}\")\n",
        "print(f\"   Test samples: {len(hf_test)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Custom Metrics for Regression with SMAPE Focus\n",
        "# ============================================================================\n",
        "def compute_metrics_for_regression(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Ensure proper shapes\n",
        "    if len(logits.shape) > 1:\n",
        "        logits = logits.flatten()\n",
        "    if len(labels.shape) > 1:\n",
        "        labels = labels.flatten()\n",
        "\n",
        "    # Calculate SMAPE (Primary metric)\n",
        "    smape = smape_metric(labels, logits)\n",
        "\n",
        "    # Calculate other metrics for monitoring\n",
        "    mse = mean_squared_error(labels, logits)\n",
        "    mae = mean_absolute_error(labels, logits)\n",
        "    r2 = r2_score(labels, logits)\n",
        "\n",
        "    # Calculate Pearson correlation\n",
        "    try:\n",
        "        pearson_corr = pearsonr(logits, labels)[0]\n",
        "    except:\n",
        "        pearson_corr = 0.0\n",
        "\n",
        "    return {\n",
        "        \"smape\": smape,  # PRIMARY METRIC\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"r2\": r2,\n",
        "        \"pearson\": pearson_corr,\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# Custom Trainer with SMAPE Loss and GPU Optimization\n",
        "# ============================================================================\n",
        "class RegressionTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        # Handle different logits shapes\n",
        "        if logits.dim() > 1:\n",
        "            logits = logits.squeeze(-1) if logits.size(-1) == 1 else logits[:, 0]\n",
        "        loss_fct = nn.MSELoss()\n",
        "        # Calculate the loss\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Print learning progress at each epoch with GPU memory info\"\"\"\n",
        "        super().on_epoch_end(args, state, control, **kwargs)\n",
        "        if state.epoch is not None and torch.cuda.is_available():\n",
        "            # Print GPU memory usage\n",
        "            gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
        "            gpu_memory_max = torch.cuda.max_memory_allocated() / 1e9\n",
        "            print(f\"   GPU Memory: {gpu_memory:.1f}GB (Peak: {gpu_memory_max:.1f}GB)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Training Arguments with GPU Optimization\n",
        "# ============================================================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta-product-pricing-test\" if TEST_MODE else \"./roberta-product-pricing\",  # CHANGED OUTPUT DIR\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"smape\",\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10 if TEST_MODE else 50,\n",
        "    report_to=None,\n",
        "    remove_unused_columns=False,\n",
        "    save_safetensors=False,\n",
        "\n",
        "    # GPU OPTIMIZATIONS\n",
        "    fp16=torch.cuda.is_available(),  # Mixed precision for GPU\n",
        "    dataloader_pin_memory=True,      # Faster data transfer to GPU\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    dataloader_prefetch_factor=2 if torch.cuda.is_available() else None,\n",
        "\n",
        "    # Evaluation optimizations\n",
        "    eval_steps=50 if TEST_MODE else 200,\n",
        "    save_steps=50 if TEST_MODE else 200,\n",
        "\n",
        "    # Gradient optimizations\n",
        "    gradient_accumulation_steps=1,\n",
        "    warmup_steps=100 if TEST_MODE else 500,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# Initialize Trainer with Early Stopping\n",
        "# ============================================================================\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    early_stopping_threshold=EARLY_STOPPING_THRESHOLD\n",
        ")\n",
        "\n",
        "trainer = RegressionTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_train,\n",
        "    eval_dataset=hf_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics_for_regression,\n",
        "    callbacks=[early_stopping_callback],\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# Training with GPU Monitoring\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ STARTING RoBERTa TRAINING\" + \" (TEST MODE)\" if TEST_MODE else \"\")  # CHANGED TEXT\n",
        "print(\"=\" * 70)\n",
        "print(f\"üéØ PRIMARY METRIC: SMAPE\")\n",
        "print(f\"‚ö° DEVICE: {device}\")\n",
        "print(f\"üìä Training samples: {len(hf_train)}\")\n",
        "print(f\"üìä Validation samples: {len(hf_val)}\")\n",
        "print(f\"üî¢ Batch size: {BATCH_SIZE} (per device) - Reduced for RoBERTa-large\")\n",
        "print(f\"üîÑ Epochs: {EPOCHS}\")\n",
        "print(f\"üí° Mixed Precision: {training_args.fp16}\")\n",
        "print(f\"üìà Early stopping: {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "try:\n",
        "    # Clear GPU cache before training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"‚úÖ GPU cache cleared before training\")\n",
        "\n",
        "    train_results = trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\nüíæ Saving model...\")\n",
        "    model_save_path = \"/content/drive/MyDrive/Arun_code/Amazon_Ml_2025/roberta-product-pricing-test\" if TEST_MODE else \"/content/drive/MyDrive/Colab Notebooks/Amazon_Ml_2025/roberta-product-pricing-final\"  # CHANGED PATH\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"‚úÖ RoBERTa model saved to: {model_save_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üíæ GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.1f}GB\")\n",
        "    raise e\n",
        "\n",
        "# ============================================================================\n",
        "# Prediction and Evaluation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREDICTION AND EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"Making predictions...\")\n",
        "test_predictions = trainer.predict(hf_test)\n",
        "predicted_prices = test_predictions.predictions.flatten()\n",
        "predicted_prices = np.maximum(predicted_prices, 0.01)  # Ensure positive prices\n",
        "\n",
        "# Create submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'sample_id': test_df['sample_id'],\n",
        "    'price': predicted_prices\n",
        "})\n",
        "\n",
        "submission_path = \"/content/drive/MyDrive/Arun_code/Amazon_Ml_2025/submission_roberta_test.csv\" if TEST_MODE else \"/content/drive/MyDrive/Colab Notebooks/Amazon_Ml_2025/submission_roberta.csv\"  # CHANGED PATH\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"‚úÖ RoBERTa submission saved: {submission_path}\")\n",
        "\n",
        "# Final metrics\n",
        "print(\"\\nüìä FINAL VALIDATION METRICS:\")\n",
        "val_predictions = trainer.predict(hf_val)\n",
        "val_metrics = compute_metrics_for_regression((val_predictions.predictions, val_predictions.label_ids))\n",
        "\n",
        "print(f\"üéØ SMAPE: {val_metrics['smape']:.2f}%\")\n",
        "print(f\"üìà MAE: ${val_metrics['mae']:.2f}\")\n",
        "print(f\"üìä R¬≤: {val_metrics['r2']:.4f}\")\n",
        "print(f\"üîó Pearson: {val_metrics['pearson']:.4f}\")\n",
        "\n",
        "# GPU memory summary\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüíæ GPU MEMORY SUMMARY:\")\n",
        "    print(f\"   Peak memory usage: {torch.cuda.max_memory_allocated() / 1e9:.1f}GB\")\n",
        "    print(f\"   Current memory usage: {torch.cuda.memory_allocated() / 1e9:.1f}GB\")\n",
        "\n",
        "print(\"\\n‚úÖ RoBERTa TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ccdd58a9f8284117a152717938deb633",
            "a12cab8cc2c546e48dbfc47e23e8e4d0",
            "a621e38ad0e94a0f86c4974eaf2e9a68",
            "d7f1caba9b204d09820f80eb68d9d2db",
            "faf8090d40b54652961c7ccffaad34f5",
            "ec1f5969610a42dc9a867cbbf625310b",
            "6d2918fd12894eef9c6c9aa7d898e40b",
            "300d36477cb240bd80171725f6fd1eae",
            "607d3ea6a35a426fbe99fbfff0bcf260",
            "70d909d79b7e42789d0a583ffa314225",
            "7c17f00c171a480694606515f44fe1c1",
            "311e7b8082234e4ab0a15d2d9d21e569",
            "784fa4397ef14766a3e6d220a5516b8c",
            "681238034d714235a78d29effc7816f6",
            "fccbccfb5f2f4b8fbc2f4f63f97a849e",
            "94f15e022d8e42a488282ef787c26c44",
            "7ba8a5d03ae5438c849ec190b1bb6ad6",
            "47c91eb90ed941a08eada523f560a5f2",
            "927bbc731ea44bb5908f4be7c5d3f901",
            "4659bfb00af148f38ccfde36df4d9f4a",
            "f209722fed404dad957de068aa44dfb7",
            "f922ef0d5542412c8994ef46a0c4de89",
            "bb9feca5a81f42f9b489b415c6103a6d",
            "3a270ded3c87413bad2c1998b383d9a2",
            "3b0be2923c0d462384a4d39de396c1e3",
            "f2d3e8cc307348aba830c59cfdf53c87",
            "b6e0c6ba206a4658923fe6238de9403b",
            "b535762d6e9743c08414aca9ef52965e",
            "8eb255697ab044aab22e11d77fe355f1",
            "4b96ec92c648469eb36f8a089114941b",
            "26c8a67b2b244fd79ab056b0dd235f40",
            "e1a37c386b204ea7977a5a62f7ab7da7",
            "a8f98da8807642df8a8a6d7c16b8f1d4"
          ]
        },
        "id": "8SzesUUPhwtM",
        "outputId": "537778f4-7393-4211-da1c-4478610fe9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GPU DETECTION AND CONFIGURATION\n",
            "======================================================================\n",
            "‚úÖ Running in Google Colab\n",
            "‚úÖ GPU: NVIDIA A100-SXM4-40GB, 40960 MiB\n",
            "‚úÖ PyTorch GPU: NVIDIA A100-SXM4-40GB\n",
            "‚úÖ GPU Memory: 42.5 GB\n",
            "‚úÖ CUDA Version: 12.6\n",
            "‚úÖ Using device: cuda\n",
            "‚úÖ GPU optimizations enabled: TF32, CuDNN benchmark\n",
            "‚úÖ GPU cache cleared\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "DATA LOADING AND PREPARATION\n",
            "======================================================================\n",
            "Loading datasets...\n",
            "Training data shape: (75000, 4)\n",
            "Test data shape: (75000, 3)\n",
            "\n",
            "Data Overview:\n",
            "Price statistics - Min: $0.13, Max: $2796.00, Mean: $23.65\n",
            "Catalog content length - Avg: 908.9 chars\n",
            "\n",
            "Loading RoBERTa tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RoBERTa tokenizer loaded successfully!\n",
            "\n",
            "======================================================================\n",
            "MODEL INITIALIZATION\n",
            "======================================================================\n",
            "\n",
            "Initializing RoBERTa model on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä MODEL ARCHITECTURE:\n",
            "   Model: RoBERTa-large\n",
            "   Device: cuda\n",
            "   Unfrozen layers: Last 2 transformer layers\n",
            "   Total parameters: 355,663,873\n",
            "   Trainable parameters: 25,496,577\n",
            "   Percentage trainable: 7.17%\n",
            "   Loss function: SMAPE (competition metric)\n",
            "   Hidden size: 1024\n",
            "‚úÖ RoBERTa model successfully moved to cuda\n",
            "\n",
            "üìä DATASET SPLIT:\n",
            "   Training samples: 60000\n",
            "   Validation samples: 15000\n",
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccdd58a9f8284117a152717938deb633"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "311e7b8082234e4ab0a15d2d9d21e569"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/75000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9feca5a81f42f9b489b415c6103a6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset preparation complete:\n",
            "   Training samples: 60000\n",
            "   Validation samples: 15000\n",
            "   Test samples: 75000\n",
            "\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üéØ PRIMARY METRIC: SMAPE\n",
            "‚ö° DEVICE: cuda\n",
            "üìä Training samples: 60000\n",
            "üìä Validation samples: 15000\n",
            "üî¢ Batch size: 64 (per device) - Reduced for RoBERTa-large\n",
            "üîÑ Epochs: 70\n",
            "üí° Mixed Precision: True\n",
            "üìà Early stopping: 7 epochs\n",
            "üéÆ GPU: NVIDIA A100-SXM4-40GB\n",
            "üíæ GPU Memory: 42.5 GB\n",
            "‚úÖ GPU cache cleared before training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40334' max='65660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40334/65660 2:23:30 < 1:30:06, 4.68 it/s, Epoch 43/70]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Smape</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Mae</th>\n",
              "      <th>R2</th>\n",
              "      <th>Pearson</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1269.808900</td>\n",
              "      <td>1710.490967</td>\n",
              "      <td>76.948967</td>\n",
              "      <td>1710.490967</td>\n",
              "      <td>17.391657</td>\n",
              "      <td>-0.133372</td>\n",
              "      <td>-0.002908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>760.452500</td>\n",
              "      <td>1415.008301</td>\n",
              "      <td>63.190819</td>\n",
              "      <td>1415.008179</td>\n",
              "      <td>14.833452</td>\n",
              "      <td>0.062415</td>\n",
              "      <td>0.335110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>691.934400</td>\n",
              "      <td>1265.112183</td>\n",
              "      <td>59.144348</td>\n",
              "      <td>1265.112305</td>\n",
              "      <td>13.701827</td>\n",
              "      <td>0.161736</td>\n",
              "      <td>0.435843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>656.565600</td>\n",
              "      <td>1182.890137</td>\n",
              "      <td>57.991920</td>\n",
              "      <td>1182.890137</td>\n",
              "      <td>13.230284</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>0.477398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>898.152900</td>\n",
              "      <td>1139.463501</td>\n",
              "      <td>55.983829</td>\n",
              "      <td>1139.463501</td>\n",
              "      <td>12.488066</td>\n",
              "      <td>0.244991</td>\n",
              "      <td>0.511365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>653.938000</td>\n",
              "      <td>1095.244385</td>\n",
              "      <td>56.484688</td>\n",
              "      <td>1095.244385</td>\n",
              "      <td>12.597808</td>\n",
              "      <td>0.274290</td>\n",
              "      <td>0.525517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>546.665000</td>\n",
              "      <td>1059.977539</td>\n",
              "      <td>57.738258</td>\n",
              "      <td>1059.977539</td>\n",
              "      <td>12.704295</td>\n",
              "      <td>0.297658</td>\n",
              "      <td>0.546925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>797.624800</td>\n",
              "      <td>1037.531860</td>\n",
              "      <td>54.950768</td>\n",
              "      <td>1037.531860</td>\n",
              "      <td>11.922587</td>\n",
              "      <td>0.312531</td>\n",
              "      <td>0.561369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>401.401900</td>\n",
              "      <td>1030.128052</td>\n",
              "      <td>54.925694</td>\n",
              "      <td>1030.128052</td>\n",
              "      <td>12.171506</td>\n",
              "      <td>0.317437</td>\n",
              "      <td>0.563478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>511.356200</td>\n",
              "      <td>1019.370422</td>\n",
              "      <td>53.948837</td>\n",
              "      <td>1019.370483</td>\n",
              "      <td>12.144215</td>\n",
              "      <td>0.324565</td>\n",
              "      <td>0.574141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>723.342200</td>\n",
              "      <td>1009.215454</td>\n",
              "      <td>53.165020</td>\n",
              "      <td>1009.215515</td>\n",
              "      <td>11.759675</td>\n",
              "      <td>0.331293</td>\n",
              "      <td>0.578167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>753.060900</td>\n",
              "      <td>1002.158142</td>\n",
              "      <td>55.396980</td>\n",
              "      <td>1002.158142</td>\n",
              "      <td>12.240871</td>\n",
              "      <td>0.335969</td>\n",
              "      <td>0.581297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>322.123200</td>\n",
              "      <td>1021.927612</td>\n",
              "      <td>52.289001</td>\n",
              "      <td>1021.927612</td>\n",
              "      <td>11.225513</td>\n",
              "      <td>0.322870</td>\n",
              "      <td>0.573377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>343.546000</td>\n",
              "      <td>994.290771</td>\n",
              "      <td>52.220703</td>\n",
              "      <td>994.290710</td>\n",
              "      <td>11.331544</td>\n",
              "      <td>0.341182</td>\n",
              "      <td>0.587139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>334.533900</td>\n",
              "      <td>991.501648</td>\n",
              "      <td>52.196651</td>\n",
              "      <td>991.501648</td>\n",
              "      <td>11.479771</td>\n",
              "      <td>0.343030</td>\n",
              "      <td>0.591357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>333.913500</td>\n",
              "      <td>986.267822</td>\n",
              "      <td>52.005310</td>\n",
              "      <td>986.267761</td>\n",
              "      <td>11.213599</td>\n",
              "      <td>0.346498</td>\n",
              "      <td>0.593308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>279.085300</td>\n",
              "      <td>962.245117</td>\n",
              "      <td>51.448448</td>\n",
              "      <td>962.245178</td>\n",
              "      <td>11.149460</td>\n",
              "      <td>0.362416</td>\n",
              "      <td>0.607530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>319.424200</td>\n",
              "      <td>990.724304</td>\n",
              "      <td>51.636581</td>\n",
              "      <td>990.724243</td>\n",
              "      <td>11.321728</td>\n",
              "      <td>0.343545</td>\n",
              "      <td>0.593760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>283.864100</td>\n",
              "      <td>980.248413</td>\n",
              "      <td>52.413498</td>\n",
              "      <td>980.248413</td>\n",
              "      <td>11.406275</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.598793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>305.022700</td>\n",
              "      <td>971.302551</td>\n",
              "      <td>52.175629</td>\n",
              "      <td>971.302612</td>\n",
              "      <td>11.276188</td>\n",
              "      <td>0.356414</td>\n",
              "      <td>0.601280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>263.533900</td>\n",
              "      <td>971.036255</td>\n",
              "      <td>51.016762</td>\n",
              "      <td>971.036255</td>\n",
              "      <td>10.918892</td>\n",
              "      <td>0.356591</td>\n",
              "      <td>0.603135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>198.574200</td>\n",
              "      <td>963.575073</td>\n",
              "      <td>51.205044</td>\n",
              "      <td>963.575073</td>\n",
              "      <td>10.994155</td>\n",
              "      <td>0.361535</td>\n",
              "      <td>0.604554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>222.154700</td>\n",
              "      <td>976.880920</td>\n",
              "      <td>50.832211</td>\n",
              "      <td>976.880920</td>\n",
              "      <td>10.877962</td>\n",
              "      <td>0.352718</td>\n",
              "      <td>0.599966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>278.043500</td>\n",
              "      <td>984.836243</td>\n",
              "      <td>51.022835</td>\n",
              "      <td>984.836243</td>\n",
              "      <td>10.902059</td>\n",
              "      <td>0.347447</td>\n",
              "      <td>0.599108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>173.282800</td>\n",
              "      <td>967.902954</td>\n",
              "      <td>51.885498</td>\n",
              "      <td>967.903137</td>\n",
              "      <td>11.314970</td>\n",
              "      <td>0.358667</td>\n",
              "      <td>0.603828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>620.538500</td>\n",
              "      <td>980.976135</td>\n",
              "      <td>50.408276</td>\n",
              "      <td>980.976257</td>\n",
              "      <td>10.630404</td>\n",
              "      <td>0.350005</td>\n",
              "      <td>0.600574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>224.179000</td>\n",
              "      <td>976.864868</td>\n",
              "      <td>50.862377</td>\n",
              "      <td>976.864929</td>\n",
              "      <td>11.196171</td>\n",
              "      <td>0.352729</td>\n",
              "      <td>0.609823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>245.237700</td>\n",
              "      <td>960.624939</td>\n",
              "      <td>51.113827</td>\n",
              "      <td>960.625000</td>\n",
              "      <td>11.056283</td>\n",
              "      <td>0.363489</td>\n",
              "      <td>0.611427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>180.619200</td>\n",
              "      <td>953.940552</td>\n",
              "      <td>50.585026</td>\n",
              "      <td>953.940430</td>\n",
              "      <td>10.891977</td>\n",
              "      <td>0.367919</td>\n",
              "      <td>0.609941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>251.638100</td>\n",
              "      <td>956.381592</td>\n",
              "      <td>50.393181</td>\n",
              "      <td>956.381592</td>\n",
              "      <td>10.754246</td>\n",
              "      <td>0.366301</td>\n",
              "      <td>0.613024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>204.700100</td>\n",
              "      <td>962.729492</td>\n",
              "      <td>50.147079</td>\n",
              "      <td>962.729370</td>\n",
              "      <td>10.668803</td>\n",
              "      <td>0.362095</td>\n",
              "      <td>0.607545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>171.945600</td>\n",
              "      <td>963.220459</td>\n",
              "      <td>49.756119</td>\n",
              "      <td>963.220581</td>\n",
              "      <td>10.587962</td>\n",
              "      <td>0.361769</td>\n",
              "      <td>0.606589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>150.094400</td>\n",
              "      <td>948.425476</td>\n",
              "      <td>49.914810</td>\n",
              "      <td>948.425415</td>\n",
              "      <td>10.666244</td>\n",
              "      <td>0.371573</td>\n",
              "      <td>0.612973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>152.108000</td>\n",
              "      <td>964.143005</td>\n",
              "      <td>50.170624</td>\n",
              "      <td>964.143066</td>\n",
              "      <td>10.856700</td>\n",
              "      <td>0.361158</td>\n",
              "      <td>0.608146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>180.447200</td>\n",
              "      <td>965.309998</td>\n",
              "      <td>50.107361</td>\n",
              "      <td>965.309998</td>\n",
              "      <td>10.735271</td>\n",
              "      <td>0.360385</td>\n",
              "      <td>0.611735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>184.501200</td>\n",
              "      <td>963.548523</td>\n",
              "      <td>49.645512</td>\n",
              "      <td>963.548645</td>\n",
              "      <td>10.623355</td>\n",
              "      <td>0.361552</td>\n",
              "      <td>0.606120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>137.153300</td>\n",
              "      <td>966.075562</td>\n",
              "      <td>50.039238</td>\n",
              "      <td>966.075684</td>\n",
              "      <td>10.792048</td>\n",
              "      <td>0.359878</td>\n",
              "      <td>0.604978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>129.503800</td>\n",
              "      <td>970.650574</td>\n",
              "      <td>49.660770</td>\n",
              "      <td>970.650696</td>\n",
              "      <td>10.713454</td>\n",
              "      <td>0.356846</td>\n",
              "      <td>0.607022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>132.295500</td>\n",
              "      <td>960.224854</td>\n",
              "      <td>50.048244</td>\n",
              "      <td>960.224854</td>\n",
              "      <td>10.787496</td>\n",
              "      <td>0.363754</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>119.942500</td>\n",
              "      <td>956.274719</td>\n",
              "      <td>49.709362</td>\n",
              "      <td>956.274841</td>\n",
              "      <td>10.510440</td>\n",
              "      <td>0.366372</td>\n",
              "      <td>0.611097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>144.821300</td>\n",
              "      <td>953.799561</td>\n",
              "      <td>50.130249</td>\n",
              "      <td>953.799561</td>\n",
              "      <td>10.599384</td>\n",
              "      <td>0.368012</td>\n",
              "      <td>0.613445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>122.390100</td>\n",
              "      <td>954.327820</td>\n",
              "      <td>50.153423</td>\n",
              "      <td>954.327759</td>\n",
              "      <td>10.620613</td>\n",
              "      <td>0.367662</td>\n",
              "      <td>0.613739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>163.056600</td>\n",
              "      <td>963.156921</td>\n",
              "      <td>50.956291</td>\n",
              "      <td>963.156860</td>\n",
              "      <td>10.543208</td>\n",
              "      <td>0.361812</td>\n",
              "      <td>0.615139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saving model...\n",
            "‚úÖ RoBERTa model saved to: /content/drive/MyDrive/Colab Notebooks/Amazon_Ml_2025/roberta-product-pricing-final\n",
            "\n",
            "======================================================================\n",
            "PREDICTION AND EVALUATION\n",
            "======================================================================\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RoBERTa submission saved: /content/drive/MyDrive/Colab Notebooks/Amazon_Ml_2025/submission_roberta.csv\n",
            "\n",
            "üìä FINAL VALIDATION METRICS:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ SMAPE: 49.65%\n",
            "üìà MAE: $10.62\n",
            "üìä R¬≤: 0.3616\n",
            "üîó Pearson: 0.6061\n",
            "\n",
            "üíæ GPU MEMORY SUMMARY:\n",
            "   Peak memory usage: 6.3GB\n",
            "   Current memory usage: 3.6GB\n",
            "\n",
            "‚úÖ RoBERTa TRAINING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}